\documentclass[10pt]{beamer}

%\usepackage[frenchb,english]{babel}
\usepackage[english,frenchb]{babel}
%\usepackage[english]{babel}

%\usepackage[utf8]{inputenc}

\usepackage{mathtools}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[all]{xy}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lmodern}
%\usepackage{fink}
\usepackage{empheq}
\usepackage{framed}
\usepackage{colortbl}
\usepackage{textcomp}
\usepackage{wasysym}
\usepackage{textcomp}
\usepackage{amsfonts}
\usepackage{bbold}
\usepackage{xcolor}

\usepackage{txfonts}
\usepackage{pxfonts}

\usepackage[cyr]{aeguill}
\usepackage{xspace}

%\usepackage{algorithm,algorithmic}
\usepackage{algorithm2e}

\usepackage{url}
%\usepackage{textpos}
%\RequirePackage[]{natbib}

\usepackage[latin1]{inputenc}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}

\usepackage{mathrsfs}

\DeclareMathOperator*{\argmax}{arg\,max}  % in your preamble
\DeclareMathOperator*{\argmin}{arg\,min}  % in your preamble 


\makeatletter
\def\colorizemath #1#2{%
    \expandafter\mathchardef\csname orig:math:#1\endcsname\mathcode`#1
    \mathcode`#1="8000
    \toks@\expandafter{\csname orig:math:#1\endcsname}%
    \begingroup
       \lccode`~=`#1
       \lowercase{%
    \endgroup
       \edef~{{\noexpand\color{#2}\the\toks@}}}%
   }
\@for\@tempa:=a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z\do{%
    \expandafter\colorizemath\@tempa{red}}
\@for\@tempa:=A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z\do{%
    \expandafter\colorizemath\@tempa{red}}
\@for\@tempa:=0,1,2,3,4,5,6,7,8,9\do{%
    \expandafter\colorizemath\@tempa{red}}
\makeatother

%\everymath{\color{red}} \everydisplay{\color{red}}
\setbeamercolor{math text displayed}{fg=red}
\setbeamercolor{math text inlined}{fg=red}


\beamersetaveragebackground{cyan!10}


% \usetheme{default} %-> A utiliser dans un premier temps

 %\usetheme{Hannover} %-> A utiliser dans un second temps
 \usetheme{Madrid} %-> A utiliser dans un second temps

%\setbeamertemplate{footline}[frame number]

\title[Réunion de synchronisation avec Valéo]{Mélange de distributions des valeurs extrêmes généralisées}
%\subtitle{}

\author[P. Dkengne et S. Girard]{Pascal Alain Dkengne Sielenou}

\institute[INRIA]{Travail en collaboration avec  St\'ephane Girard (INRIA)}


%\institute[INRIA]{INRIA Grenoble}

%\logo{%
%	\makebox[0.95\paperwidth]{%
%	\includegraphics[width=1cm,height=2cm,keepaspectratio]{logo-inria.png}%
%	\hfill%
%	\includegraphics[width=1cm,height=1cm,keepaspectratio]{logo-valeo.jpg}%
%	 }%
%}

%\date[\today]{\today}

\date[26 octobre 2023]{Réunion de synchronisation du 26 octobre 2023}




\begin{document}

\maketitle




%\begin{frame}
%
%\titlepage
%
%\end{frame}



%\begin{frame}
%
%\frametitle{Plan} \tableofcontents
%
%\end{frame}






%%--------------------------------------------------------------------------------------
%%--------------------------------------------------------------------------------------
%\section{Quelques résultats obtenus}
%\frame{\tableofcontents[currentsection]}
%%--------------------------------------------------------------------------------------
%%--------------------------------------------------------------------------------------

%\begin{frame}
%	\frametitle{Rappels: Loi des valeurs extremes généralisées} %Each frame should have a title.
%	
%	\small
%	
%	\begin{itemize}
%		\item Soit $X$ une variable aléatoire de fonction de répartion $F$.
%		
%		\item Soient $X_1,  \ldots  ,X_n$ des copies indépendantes de $X.$ 
%		
%		\item Soit $M_n=\max\{ X_1,  \ldots  ,X_n \},$ la suite des maximums de $X.$
%	\end{itemize}
%	
%	
%
%	\begin{theorem}
%		S'il existent des suites de constantes $a_n>0$ et $b_n\in \mathbb{R}$ telles que
%		\begin{equation}\label{eqn_mda}
%			\lim_{n \rightarrow +\infty} \mathbb{P}\left\{\frac{M_n - b_n}{a_n}\leq x \right\} = 	\lim_{n\rightarrow +\infty} F^{n}\left(a_{n}x + b_{n}\right)= G(x)  
%		\end{equation}
%		pour une fonction non dégénérée $G,$ alors $G$ appartient à la famille des valeurs extrêmes généralisées (GEV)
%		\begin{equation}
%			G(x)= G(x;\gamma,\sigma,\mu) = \exp\left\{-\left[  1+ \gamma\left( \frac{x-\mu}{\sigma} \right) \right]^{-\frac{1}{\gamma}}  \right\} ,
%		\end{equation}
%		définie sur 
%		$\left\{x \in\mathbb{R}:\, 1+ \gamma\left( \frac{x-\mu}{\sigma} \right)>0 \right\},$ où $\gamma \neq 0,\,\mu \in\mathbb{R},$ $\sigma>0.$
%	\end{theorem}
%	Si $F$ vérifie (\ref{eqn_mda}), on dit qu'il appartient au domaine d'attraction de la function $G.$
%\end{frame}





\begin{frame}
	\frametitle{} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Melange des distributions de probabilités]
		Une loi de probabilités est dite \textbf{loi de mélange} si sa fonction de répartition est une \textbf{moyenne pondérée algébrique} ou une \textbf{moyenne pondérée géométrique} de plusieurs fonctions de répartition. 
	\end{definition}
	
	\begin{example}
		Considérons une suite de $p$ fonctions de répartition $F_j,$ $j=1,\cdots,p$ et  un vecteur $\omega = \left(\omega_1,\cdots, \omega_p\right) \in [0,\, 1]^p$  tel que $\sum_{j=1}^{p}\, \omega_j = 1.$ 
		Les lois de mélange moyennes pondérées algébriquement et géométriquement ont respectivement les formes (\ref{eqn_mixing_distributions_sum}) et (\ref{eqn_mixing_distributions_prod}) ci-dessous
		\begin{equation}\label{eqn_mixing_distributions_sum}
			F_{\mathbb{S}}(x;\omega) = \sum_{j = 1}^{p} \omega_j F_j(x),
		\end{equation}
		\begin{equation}\label{eqn_mixing_distributions_prod}
			F_{\mathbb{P}}(x;\omega) = \prod_{j = 1}^{p} F_j^{\omega_j}(x).
		\end{equation}

	\end{example}
	
	
	
\end{frame}




\begin{frame}
	\frametitle{} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Mélange des distributions GEV]
		\begin{itemize}
			\item Désignons par $\omega = \left(\omega_1,\cdots, \omega_p\right) \in [0,\, 1]^p$ un vecteur tel que $\sum_{j=1}^{p}\, \omega_j = 1.$ 
			
			\item Désignons par $G_j$ une fonction de répartition de la loi GEV.
			
			\item Désignons par $\Theta = \left(\Theta_j,\, j=1,\cdots, p\right)$ où $\Theta_j=\left(\gamma_j, \sigma_j, \mu_j \right)$ est un vecteur des paramètres de la distribution GEV nommée $G_j.$
		\end{itemize}
		
		On définit le modèle de mélange $G_{\mathbb{P}}$ des lois GEV nommées $G_j$ par  
		\begin{equation}\label{mixing_gev_models}
			G_{\mathbb{P}}(x;\omega,\Theta) = \prod_{j = 1}^{p} G_j^{\omega_j}(x;\Theta_j).
		\end{equation}
		
	\end{definition}
	
    \begin{block}{}
    	Les fonctions $G_j$ sont explicitement définies par
    	\begin{equation}
    		G_j(x)= G(x;\gamma_j,\sigma_j,\mu_j) = \exp\left\{-\left[  1+ \gamma_j\left( \frac{x-\mu_j}{\sigma_j} \right) \right]^{-\frac{1}{\gamma_j}}  \right\} ,
    	\end{equation}
    	sur l'ensemble
    	$\left\{x \in\mathbb{R}:\, 1+ \gamma_j\left( \frac{x-\mu_j}{\sigma_j} \right)>0 \right\},$ où $\gamma_j \neq 0,\,\mu_j \in\mathbb{R},$ $\sigma_j>0.$
    \end{block}

    
\end{frame}






\begin{frame}
	\frametitle{} %Each frame should have a title.
	
	\small
	
	\begin{theorem}[Stabilité de la famille $\left\{ G_{\mathbb{P}}\left(\cdot; \omega, \Theta \right) \right\}$]
		Pour tout entier positif $m$ et pour tout réel $x,$ la propriété suivante est satisfaite
		\begin{equation}\label{max_stable_eqn_1}
			G^{m}_{\mathbb{P}}\left(x; \omega, \Theta \right) = \prod_{j=1}^{p}\, \left[ G_j\left(x; \Theta_{j}(m) \right) \right]^{\omega_j} = G_{\mathbb{P}}\left(x; \omega, \Theta(m) \right).
		\end{equation}
		Ici, $\Theta(m) = \left(\Theta_{j}(m),\, j=1,\cdots, p\right)$
		où
		$\Theta_{j}(m)=\left(\gamma_{j}(m), \sigma_{j}(m), \mu_{j}(m) \right)$
		
		avec
		$   
		\gamma_{j}(m) = \gamma_j,\quad  
		\sigma_{j}(m) = \sigma_j\,m^{\gamma_j},\quad 
		\mu_{j}(m) = \mu_j + \sigma_j\left(\frac{m^{\gamma_j} - 1}{\gamma_j}\right) .   
		$
	
	\end{theorem}
	
	
	\begin{block}{}
		La propriété (\ref{max_stable_eqn_1}) montre que si la loi d'une v.a. $X$ appartient à la famille des probabilités $\left\{ G_{\mathbb{P}}\left(\cdot; \omega, \Theta \right) \right\},$ alors la loi du maximum de $m$ copies indépendantes de $X$ appartient également à cette même famille de probabilités.
	\end{block}
	
	
\end{frame}




%\begin{frame}
%	\frametitle{Propriétés (2/5)} %Each frame should have a title.
%	
%	\small
%	
%	\begin{theorem}[Equivalence des queues de distributions]
%		On considère les distributions de probabilités $F_{\mathbb{S}}$ et $F_{\mathbb{P}}$ définies resp. par (\ref{eqn_mixing_distributions_sum}) et (\ref{eqn_mixing_distributions_prod}).
%		
%		S'ils existent des constantes $\tau_j > 0$ telles que pour tout $j=1,\cdots, p$ nous avons
%		\begin{equation}\label{eqn_tail_equiv_1}
%			\lim_{x \rightarrow x_{F_j}} \frac{1 - F_j(x)}{1 - G_j(x; \Theta_j)} = \tau_j,
%		\end{equation}
%		où $x_{F_j} = \sup\left\{ x\in \mathbb{R}:\,\, F_j(x) < 1\right\}$ est la borne supérieure du support de la distribution $F_j,$ alors les limites suivantes sont satisfaites
%		\begin{equation}\label{eqn_tail_equiv_2}
%			\lim_{x \rightarrow x^{\star}} \frac{1 - F_{\mathbb{S}}(x; \omega)}{1 - G_{\mathbb{P}}(x;\omega, \Theta(\tau))} = 1,
%			\quad
%			\lim_{x \rightarrow x^{\star}} \frac{1 - F_{\mathbb{P}}(x; \omega)}{1 - G_{\mathbb{P}}(x;\omega, \Theta(\tau))} = 1,
%		\end{equation}
%		où $x^{\star} = \max\{x_{F_j},\,j=1,\cdots,p\}$ et $\tau = (\tau_1,\cdots, \tau_p).$
%		
%		\textbf{}\\
%		Ici, $\Theta(\tau) = \left(\Theta_{j}(\tau_j),\, j=1,\cdots, p\right)$
%		où
%		$\Theta_{j}(\tau_j)=\left(\gamma_{j}(\tau_j), \sigma_{j}(\tau_j), \mu_{j}(\tau_j) \right)$
%		
%		\textbf{}\\	
%		avec
%		$   
%		\gamma_{j}(\tau_j) = \gamma_j,\quad  
%		\sigma_{j}(\tau_j) = \sigma_j\,\tau_j^{\gamma_j},\quad 
%		\mu_{j}(\tau_j) = \mu_j + \sigma_j\left(\frac{\tau_j^{\gamma_j} - 1}{\gamma_j}\right) .   
%		$
%
%	\end{theorem}
%	
%\end{frame}




%\begin{frame}
%	\frametitle{Preuve de la Propriétés (2/5) } %Each frame should have a title.
%	
%	\small
%	
%	\begin{block}{}
%		Pour des grandes valeurs de $x\in \mathbb{R},$ on peut effectuer les approximations suivantes
%		\begin{eqnarray*}
%			1 - F_{\mathbb{S}}(x)
%			&=& 1- \sum_{j=1}^{p}\omega_j F_j(x)
%			=\sum_{j=1}^{p}\omega_j\left(1- F_j(x)\right)\\
%			&\approx& \sum_{j=1}^{p}\omega_j \tau_j\left(1- G_j(x;\Theta_j)\right),\quad \mathrm{(faisant\,\, usage\,\, de \,\, (\ref{eqn_tail_equiv_1}))}\\
%			&=& - \sum_{j=1}^{p}\omega_j \log G_j^{\tau_j}(x;\Theta_j)
%			= - \sum_{j=1}^{p} \log G_j^{\omega_j}(x;\Theta_j(\tau_j))\\
%			&=& - \log \prod_{j=1}^{p}  G_j^{\omega_j}(x;\Theta_j(\tau_j))
%			\approx 1 - \prod_{j=1}^{p}  G_j^{\omega_j}(x;\Theta_j(\tau_j))\\
%			&=& 1 - G_{\mathbb{P}}(x;\omega, \Theta(\tau)).
%		\end{eqnarray*}
%	\end{block}
%
%
%\end{frame}




%\begin{frame}
%	\frametitle{Preuve de la Propriétés (2/5) } %Each frame should have a title.
%	
%	\small
%	
%	\begin{block}{}
%		Pour des grandes valeurs de $x\in \mathbb{R},$ on peut effectuer les approximations suivantes
%		\begin{eqnarray*}
%			1 - F_{\mathbb{P}}(x)
%			&\approx& - \log F_{\mathbb{P}}(x)
%			= - \log \prod_{j=1}^{p} F^{\omega_j}_j(x)\\
%			&=& \sum_{j=1}^{p}\omega_j\left(- \log F_j(x)\right)
%			\approx \sum_{j=1}^{p}\omega_j\left(1- F_j(x)\right)\\
%			&\approx& \sum_{j=1}^{p}\omega_j \tau_j\left(1- G_j(x;\Theta_j)\right),\quad \mathrm{(faisant\,\, usage\,\, de \,\, (\ref{eqn_tail_equiv_1}))}\\
%			&=& - \sum_{j=1}^{p}\omega_j \log G_j^{\tau_j}(x;\Theta_j)
%			= - \sum_{j=1}^{p} \log G_j^{\omega_j}(x;\Theta_j(\tau_j))\\
%			&=& - \log \prod_{j=1}^{p}  G_j^{\omega_j}(x;\Theta_j(\tau_j))
%			\approx 1 - \prod_{j=1}^{p}  G_j^{\omega_j}(x;\Theta_j(\tau_j))\\
%			&=& 1 - G_{\mathbb{P}}(x;\omega, \Theta(\tau)).
%		\end{eqnarray*}
%	\end{block}
%	
%	
%\end{frame}





%\begin{frame}
%	\frametitle{Propriétés (3/5)} %Each frame should have a title.
%	
%	\small
%	
%	\begin{theorem}
%		On considère les distributions de probabilités $F_{\mathbb{S}}$ et $F_{\mathbb{P}}$ définies resp. par (\ref{eqn_mixing_distributions_sum}) et (\ref{eqn_mixing_distributions_prod}).
%		
%		S'ils existent des suites $a_{n,j}>0$ et $b_{n,j}\in \mathbb{R}$ telles que pour tout $x\in \mathbb{R},$ nous avons
%		\begin{equation}\label{eqn_mda_1}
%			\lim_{n\rightarrow +\infty} F^{n}_{j}\left(a_{n,j}x + b_{n,j}\right) = G_j(x),\quad 
%			\lim_{n\rightarrow +\infty} n \left[1 - F_{j}\left(a_{n,j}x + b_{n,j}\right) \right] = -\log G_j(x),
%		\end{equation}
%		alors les limites suivantes sont satisfaites pour tout $x\in \mathbb{R}:$
%		\begin{equation}
%			\lim_{n\rightarrow +\infty} F^n_{\mathbb{P}}\left(v_n(x)\right) \leq G_{\mathbb{P}}(x) 
%			\leq \lim_{n\rightarrow +\infty} F^n_{\mathbb{P}}\left(u_n(x)\right),
%		\end{equation}
%		\begin{equation}
%			\lim_{n\rightarrow +\infty} F^n_{\mathbb{S}}\left(v_n(x)\right) \leq G_{\mathbb{P}}(x) 
%			\leq \lim_{n\rightarrow +\infty} F^n_{\mathbb{S}}\left(u_n(x)\right),
%		\end{equation}
%		où les suites $u_n(x)$ et $v_n(x)$ sont définies par
%		\begin{equation}
%			u_n(x) = \max_{1\leq j\leq p}\left\{ a_{n,j} x + b_{n,j}\right\},\quad 
%			v_n(x) = \min_{1\leq j\leq p}\left\{ a_{n,j} x + b_{n,j}\right\}.
%		\end{equation}
%		De plus, il existe deux suites non linéaires $s_n(x), t_n(x) \in [v_n(x),\,u_n(x)]$ qui sont strictement croissantes  et satisfont
%		\begin{equation}
%			\lim_{n\rightarrow +\infty} F^n_{\mathbb{P}}\left(s_n(x)\right) = G_{\mathbb{P}}(x)
%			= \lim_{n\rightarrow +\infty} F^n_{\mathbb{S}}\left(t_n(x)\right).
%		\end{equation}
%		
%		
%		
%	\end{theorem}
%	
%\end{frame}






\begin{frame}
	\frametitle{} %Each frame should have a title.
	
	\small
	
	
	\begin{block}{Distribution des extrêmes et mélange des lois GEV}
		\begin{itemize}
			\item Soit $X$ une v.a. de fonction de répartition $F$ et de borne supérieure $x_F.$
			
			\item Soient $b_1,\cdots,b_p$ une suite de $p$ entiers positifs suffisamment grands.
			
			\item On suppose que pour tout $j=1,\cdots,p$ et pour toute grande valeur  $x\in \mathbb{R},$ l'équivalence suivante est satisfaite
			\begin{equation}\label{gev_approx_eqn_1}
				(\mathbb{P}\{X \leq x\})^{b_j} = (F(x))^{b_j} \sim G_j(x;\Theta_j),
			\end{equation}
			où $G_j$ est une distribution GEV de paramètre $\Theta_j=\left(\gamma_j,\sigma_j,\mu_j\right)\in \mathbb{R}^3.$
		\end{itemize}
	    Alors,  quel que soit le vecteur $\omega = \left(\omega_1,\cdots, \omega_p\right) \in [0,\, 1]^p$  tel que $\sum_{j=1}^{p}\, \omega_j = 1$ et pour toute grande valeur $x \in \mathbb{R},$ on peut faire l'approximation suivante
	    \begin{equation}\label{marginal_distribution_eqn_1}
	    	\mathbb{P}\{X \leq x\} = F(x) \sim 
	    	\prod_{j=1}^{p}\, \left[ G_j\left(x; \Theta_{j}(b_j) \right) \right]^{\omega_j} = G_{\mathbb{P}}\left(x; \omega, \Theta(b) \right).
	    \end{equation}
	    Ici, $b=(b_1,\cdots,b_p),$ $\Theta(b) = \left(\Theta_{j}(b_j),\, j=1,\cdots, p\right)$
	    où
	    $\Theta_{j}(b_j)=\left(\gamma_{j}(b_j), \sigma_{j}(b_j), \mu_{j}(b_j) \right)$
	    
	    \textbf{}\\
	    avec
	    $   
	    \gamma_{j}(b_j) = \gamma_j,\quad  
	    \sigma_{j}(b_j) = \sigma_j\,b_j^{-\gamma_j},\quad 
	    \mu_{j}(b_j) = \mu_j + \sigma_j\left(\frac{b_j^{-\gamma_j} - 1}{\gamma_j}\right).   
	    $
	\end{block}
	
	
\end{frame}




%\begin{frame}
%	\frametitle{} %Each frame should have a title.
%	
%	\small
%	
%	\begin{block}{Preuve de la Propriétés (4/5)}
%		Pour tout $j=1,\cdots,p$ et pour toute grande valeur $x\in \mathbb{R},$ la formule (\ref{gev_approx_eqn_1}) permet d'écrire
%		\begin{equation*}
%			\mathbb{P}\{X\leq x\} = F(x) \sim G_j^{1/b_j}(x;\Theta_j) = G_j(x;\Theta_j(b_j)).
%		\end{equation*}
%		L'approximation $\mathbb{P}\{X \leq x\} \approx G_{j}(x;\Theta_j(b_j))$ ainsi obtenue permet d'écrire
%		\begin{eqnarray*}
%			\mathbb{P}\{X \leq x\}
%			&=&
%			\left(\mathbb{P}\{X \leq x\}\right)^{\sum_{j=1}^{p}\omega_j}
%			=
%			\prod_{j=1}^{p}\left(\mathbb{P}\{X \leq x\}\right)^{\omega_j}\\
%			&\sim&
%			\prod_{j=1}^{p}\, \left[ G_j\left(x; \Theta_{j}(b) \right) \right]^{\omega_j}
%			=
%			G_{\mathbb{P}}\left(x; \omega, \Theta(b) \right).
%		\end{eqnarray*}
%
%		
%	\end{block}
%	
%	
%\end{frame}







%\begin{frame}
%	\frametitle{Propriétés (5/5)} %Each frame should have a title.
%	
%	
%	\small
%	
%	\begin{theorem}
%		La variable aléatoire $X$ associée à la fonction de répartition $G_{\mathbb{P}}(x)=\prod_{j=1}^{p}  G^{\omega_j}_j(x)$ est définie par
%		\begin{equation}\label{eqn_sim}
%			X = \max_{1\leq j\leq p}\left\{ G_j^{-1}\left(U_j^{1/\omega_j}\right) \right\},
%		\end{equation}
%		où $U_j$ est une variable aléatoire uniform sur $[0,\,1].$
%	\end{theorem}
%	
%	\begin{block}{Simulation}
%		La formule (\ref{eqn_sim}) suggère les deux étapes suivantes pour simuler une observation $x$ de la distribution $G_{\mathbb{P}}.$
%		\begin{enumerate}
%			\item Générer les valeurs indépendantes $u_j,$ $j=1,2,\cdots, p$ à l'aide d'une distribution uniforme sur $[0,\,1].$
%			
%			\item Prendre comme valeur simulée de la distribution $G_{\mathbb{P}}$ la quantité $x$ définie par
%			\begin{equation}
%			x = \max_{1\leq j\leq p}\left\{ G_j^{-1}\left(u_j^{1/\omega_j}\right) \right\}.
%			\end{equation}
%		\end{enumerate}
%	\end{block}	
%	
%\end{frame}





\begin{frame}
	\frametitle{Modélisation des valeurs extrêmes (1/2)} %Each frame should have a title.
	
	\small
	
	Soit $\mathcal{X}=(x_1, \ldots ,x_n)$ un échantillon d'une v.a. $X$ de distribution de probabilités $F.$
	
	\begin{block}{Estimation du paramètre $\Theta$}
		
		\begin{enumerate}
			\item Soit $b=\left\{b_j\in \mathbb{N}^{\star},\, j=1,\cdots, p\right\}$ un ensemble de tailles de blocs assez grandes.
			
			\item Pour chaque taille de blocs $b_j\in b,$ partitionner l'échantillon $\mathcal{X}$ en $n\left(b_j\right)=\lfloor n/b_j\rfloor$ blocs disjoints contenant $b_j$ observations consécutives.
			
			\item Désignons par $z_{b_j}=\left(z_{b_j,1},\cdots, z_{b_j,n\left(b_j\right)}\right)$ l'échantillon des maximums où $z_{b_j,i}$ est le maximum des observations du $i$-th bloc de taille $b_j.$
			
			\item Soit $\widehat{\Theta}_j=\left(\widehat{\gamma}_j, \widehat{\sigma}_j, \widehat{\mu}_j\right)$ les paramètres de la loi GEV nommée $G_j$ estimés sur l'échantillon des maximums $z_{b_j}.$
			
			\item Pour des grandes valeurs de $x\in \mathbb{R},$ la formule (\ref{marginal_distribution_eqn_1}) permet de faire l'approximation
			$
			\mathbb{P}\{X \leq x\} \approx 
			G_{\mathbb{P}}\left(x; \omega, \widehat{ \Theta}(b) \right) =
			\prod_{j=1}^{p}\, \left[ G_j\left(x; \widehat{\Theta}_{j}(b_j) \right) \right]^{\omega_j},
			$			
			où les composantes du vecteur $\widehat{\Theta}_j(b_j) = \left(\widehat{\gamma}_j(b_j), \widehat{\sigma}_j(b_j), \widehat{\mu}_j(b_j)\right)$ constituant le paramètre $\widehat{\Theta}(b)$ s'écrivent
			$   
			\widehat{\gamma}_{j}(b_j) = \widehat{\gamma}_j,\quad  
			\widehat{\sigma}_{j}(b_j) = \widehat{\sigma}_j\,b_j^{-\widehat{\gamma}_j},
			\quad
			\mu_{j}(b_j) = \widehat{\mu}_j + \widehat{\sigma}_j\left(\frac{b_j^{-\widehat{\gamma}_j} - 1}{\widehat{\gamma}_j}\right).   
			$
		\end{enumerate}
	
	\end{block}
	
\end{frame}







\begin{frame}
	\frametitle{Modélisation des valeurs extrêmes (2/2)} %Each frame should have a title.
	
	\small
	
		\begin{block}{Estimation du paramètre $\omega$}
		Le vecteur $\omega$ des poids de la loi des extrêmes $G_{\mathbb{P}}\left(x; \omega, \widehat{\Theta}(b) \right)$ peut être estimé en résolvant le problème d'optimisation suivant
		\begin{equation}
			\widehat{\omega} = \argmin_{\omega}\left\{ \sum_{x\in \mathcal{X}, x>x(\alpha)}\, \left[F_{X,n}(x) - G_{\mathbb{P}}\left(x; \omega, \widehat{\Theta}(b) \right)\right]^2\right\},
		\end{equation}
		où $F_{X,n}$ est la fonction de répartition empirique de la v.a. $X$ estimée sur l'échantillon $\mathcal{X}$  de même que le quantile empirique $x(\alpha)$ d'ordre $\alpha > 0.5.$ 
	\end{block}
	
	
	\begin{block}{Estimation des quantiles extrêmes}
		Soit $\alpha\in [0,\,1]$ tel que $\alpha$ tend vers $0.$
		
		Le quantile extrême $x(\alpha)$ défini par 
		$\mathbb{P}\left\{ X > x(\alpha)\right\} = \alpha $
		peut être estimé par une quantité $\widehat{x}(\alpha)$ qui est solution numérique de l'équation 
		$
		G_{\mathbb{P}}\left(x; \widehat{\omega}, \widehat{\Theta}(b) \right) = 1 - \alpha .
		$
	\end{block}
	
	\textbf{\color{red} Conclusion:} \textit{ \color{blue} Ce travail explique comment combiner plusieurs modèles GEV pour obtenir un modèle assez précis dans le calcul des quantiles extrêmes d'une v.a.}
	
\end{frame}





%\begin{frame}
%	\frametitle{Modélisation des valeurs extrêmes (3/3)} %Each frame should have a title.
%	
%	\small
%	
%	\begin{example}[Estimation du paramètre $\omega$ (péssimiste)]
%		
%		
%		
%		
%		\begin{itemize}
%			\item Ce poids favorise les modèles $G_j$ ayant des grands paramètres et pénalise ceux ayant des petits paramètres.
%			
%			\item Un tel poids permet de réduire le risque d'obtenir un modèle de mélange $G_{\mathbb{P}}$ qui sous estime les quantiles extrêmes.
%		\end{itemize}
%		
%		
%		
%	\end{example}
%	
%	
%	
%	
%	
%\end{frame}







%\begin{frame}
%	\frametitle{Quantile et distribution de mélange (1/2)} %Each frame should have a title.
%	
%	\small
%	
%	\begin{theorem}
%		Soit $\omega = \left(\omega_1,\cdots, \omega_p\right) \in [0,\, 1]^p$ un vecteur de poids. C'est-à-dire $\sum_{j=1}^{p}\, \omega_j = 1.$ 
%		
%		On considère les distributions de probabilités $F_{\mathbb{S}}$ et $F_{\mathbb{P}}$ définies par
%		$$
%		F_{\mathbb{S}}(x;\omega) = \sum_{j = 1}^{p} \omega_j F_j(x),\quad
%		F_{\mathbb{P}}(x;\omega) = \prod_{j = 1}^{p} F_j^{\omega_j}(x)
%		$$
%		Si $x_j(\alpha)$ désigne le quantile d'ordre $\alpha\in [0,\,1]$ de la distribution $F_j,$ i.e. $F_j(x_j(\alpha)) = 1 - \alpha,$ alors les quantités
%		$x_1(\alpha) = \min_{1\leq j \leq p}\left\{ x_j(\alpha)\right\}$
%		$x_2(\alpha) = \max_{1\leq j \leq p}\left\{ x_j(\alpha)\right\}$ satisfont pour tout vecteur de poids $\omega$ les inégalités suivantes:
%		\begin{itemize}
%			\item \begin{center}
%				$F_{\mathbb{S}}(x_1(\alpha);\omega) \leq 1 - \alpha \leq F_{\mathbb{S}}(x_2(\alpha);\omega),$
%			\end{center}
%			
%			\item \begin{center}
%				$
%			F_{\mathbb{P}}(x_1(\alpha);\omega) \leq 1 - \alpha \leq F_{\mathbb{P}}(x_2(\alpha);\omega).$
%			\end{center}
%		\end{itemize}
%	\end{theorem}
%	
%\end{frame}
	 
	
	
	
%\begin{frame}
%	\frametitle{Quantile et distribution de mélange (2/2)} %Each frame should have a title.
%	
%	\small
%	
%	\begin{block}{Preuve}
%		Par hypothèse, pour tout $j=1, \cdots, p$ on a les deux égalités suivantes vraies
%		$$F_j^{\omega_j}(x_j(\alpha)) = \alpha^{\omega_j},\quad \omega_j F_j(x_j(\alpha)) = \omega_j \alpha.$$
%		Par définition, pour tout $j=1, \cdots, p$ on a l'inégalité suivante:
%		$$  F_j\left(\max_{1\leq j \leq p}\left\{ x_j(\alpha)\right\}\right) \geq F_j(x_j(\alpha)). $$
%		Ce qui implique
%		$$ F_j^{\omega_j}(x(\alpha)) \geq \alpha^{\omega_j},\quad \omega_j F_j(x(\alpha)) \geq \omega_j \alpha.    $$
%		On en déduit que
%		$$ F_{\mathbb{P}}(x(\alpha)) = \prod_{j = 1}^{p} F_j^{\omega_j}(x(\alpha)) \geq \prod_{j = 1}^{p} \alpha^{\omega_j} = \alpha,\quad 
%		F_{\mathbb{S}}(x(\alpha)) = \sum_{j = 1}^{p} \omega_j F_j(x(\alpha)) \geq \sum_{j = 1}^{p} \omega_j \alpha = \alpha.  $$
%	\end{block}	
%			
%\end{frame}





\begin{frame}
	\frametitle{Modèles de mélange des distributions GEV: Cas IID} %Each frame should have a title.
	
	\small
	
	Soit $X$ une variable aléatoire ayant une distribution de probabilité \textbf{inconnue}.
	
	\begin{block}{}
		\begin{itemize}
			\item Soit $b=\left\{b_j\in \mathbb{N}^{\star},\, j=1,\cdots, p\right\}$ un ensemble de $p$ tailles de blocs assez grandes.
			
			\item Soit $\left(\gamma_j, \sigma_j, \mu_j\right)$ le vecteur des paramètres de la loi GEV nommée $G_j(\cdot)$ caractérisant la distribution des maximums de $b_j$ obs. consécutives de la v.a. $X.$
		\end{itemize}
	\end{block}
	
	
	\begin{definition}[Lois GEV normalisées]
		La loi GEV normalisée $G_j^{1/b_j}(\cdot)$ caractérisant la distribution des grandes obs. de la v.a. $X$ est une loi GEV $G_j(\cdot; \gamma_{j}(b_j),\,\sigma_{j}(b_j),\,\mu_{j}(b_j) )$ dont les trois paramètres sont définis par: 
		\begin{itemize}
			\item $\gamma_{j}(b_j) = \gamma_j$ (paramètre de forme),
			
			\item $\sigma_{j}(b_j) = \sigma_j\,\left(1/b_j\right)^{\gamma_j}$ (paramètre d'échelle),
			
			\item $\mu_{j}(b_j) = \mu_j + \sigma_j\left(\frac{\left(1/b_j\right)^{\gamma_j} - 1}{\gamma_j}\right)$ (paramètre de position).
		\end{itemize}
		
	\end{definition}
	
	
\end{frame}



\begin{frame}
	\frametitle{Modèles de mélange des distributions GEV: Cas IID} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Modèle de mélange $G_{\mathbb{P}}(\cdot; \omega)$]
		Le modèle de mélange $G_{\mathbb{P}}(\cdot; \omega)$ est défini pour tout $x\in \mathbb{R}$ par 
		\begin{equation}
			G_{\mathbb{P}}(x; \omega) = \prod_{j = 1}^{p} G_j^{\omega_j}\left(x;\gamma_j(b_j), \sigma_j(b_j), \mu_j(b_j)\right),
		\end{equation}
		où $\omega = \left(\omega_1,\cdots, \omega_p\right) \in [0,\, 1]^p$ est un vecteur des poids. 
	\end{definition}
	
	\begin{block}{Estimation des poids}
		\begin{equation}
			\widehat{\omega} = \argmin_{\omega}\left\{ \sum_{x\in \mathcal{X}, x>x_{n, \alpha}}\, \left[F_{n}(x) - G_{\mathbb{P}}\left(x; \omega \right)\right]^2\right\},
		\end{equation}
		où $F_{n}(\cdot)$ est la fonct. de répartition empirique estimée sur une séq. $\mathcal{X} = \{x_1,\cdots, x_n\}$ d'obs. de $X$ de même que le quantile empirique $x_{n, \alpha}$ d'ordre $\alpha > 0.5.$
	\end{block}
	
	
	
	
\end{frame}



\begin{frame}
	\frametitle{Modèles de mélange des distributions GEV: Cas IID} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Modèle de mélange $G_{\mathbb{M}}(\cdot; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu})$]
		Soit $G(\cdot)$ la fonction de répartition de la loi GEV. Le modèle de mélange $G_{\mathbb{M}}(\cdot; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu})$ est défini pour tout $x\in \mathbb{R}$ par 
		\begin{equation}
			G_{\mathbb{M}}\left(x; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu} \right) = G\left(x;\sum_{j = 1}^{p}\omega_{\gamma,j}\cdot\gamma_j(b_j), \sum_{j = 1}^{p}\omega_{\sigma,j}\cdot\sigma_j(b_j), \sum_{j = 1}^{p}\omega_{\mu,j}\cdot\mu_j(b_j)\right),
		\end{equation}
		où $\omega_{\gamma} = \left(\omega_{\gamma, 1},\cdots, \omega_{\gamma, p}\right) \in [0,\, 1]^p,$ $\omega_{\sigma} = \left(\omega_{\sigma, 1},\cdots, \omega_{\sigma, p}\right) \in [0,\, 1]^p$ et $\omega_{\mu} = \left(\omega_{\mu, 1},\cdots, \omega_{\mu, p}\right) \in [0,\, 1]^p$ sont trois vecteurs des poids. 
	\end{definition}

	\begin{block}{Estimation des poids}
		\begin{equation}
			\left(\widehat{\omega}_{\gamma},
			\widehat{\omega}_{\sigma},
			\widehat{\omega}_{\mu}\right) = \argmin_{\omega_{\gamma},\omega_{\sigma}, \omega_{\mu}}\left\{ \sum_{x\in \mathcal{X}, x>x_{n, \alpha}}\, \left[F_{n}(x) - G_{\mathbb{M}}\left(x; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu} \right)\right]^2\right\},
		\end{equation}
		où $F_{n}(\cdot)$ est la fonct. de répartition empirique estimée sur une séq. $\mathcal{X} = \{x_1,\cdots, x_n\}$ d'obs. de $X$ de même que le quantile empirique $x_{n, \alpha}$ d'ordre $\alpha > 0.5.$
	\end{block}
	
\end{frame}


\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Stationnaire} %Each frame should have a title.
	
	\small
	
	Soit $X_t,$ $t=1, 2, \cdots$ une série temporelle  \textbf{stationnaire} de processus aléatoire \textbf{inconnu}.
	
	\begin{block}{}
		\begin{itemize}
			\item Soit $b=\left\{b_j\in \mathbb{N}^{\star},\, j=1,\cdots, p\right\}$ un ensemble de $p$ tailles de blocs assez grandes.
			
			\item Soit $\left(\gamma_j, \sigma_j, \mu_j\right)$ le vecteur des paramètres de la loi GEV nommée $G_j(\cdot)$ caractérisant la distribution des maximums de $b_j$ obs. consécutives de la serie $X_t.$
			
			\item Soit $\theta_j\in [0,\,1]$ l'\textbf{indice extremal} associé au seuil défini par le quantile empirique $x_{n, 1/b_j}$ d'ordre $1/b_j$ estimé sur une séquence $\mathcal{X}_n = \{x_1,\cdots, x_n\}$ de la série $X_t.$
			
		\end{itemize}
	\end{block}
	
	
	\begin{block}{Indice extremal}
		Un indice extremal $\theta$ quantifie le degré de dépendance entre l'occurrence des valeurs extrêmes consécutives. Cette dépendance est \textbf{forte} lorsque $\theta$ tend vers $0$ et \textbf{faible} lorsque $\theta$ tend vers $1.$
	\end{block}
	
	\begin{definition}[Lois GEV normalisées]
		La loi GEV normalisée $G_j^{\theta_j/b_j}(\cdot)$ caractérisant la distribution des grandes obs. de $X_t$ est une loi GEV $G_j\left(\cdot;\gamma_j(b_j,\theta_j),\, \sigma_j(b_j,\theta_j),\, \mu_j(b_j, \theta_j)\right)$ dont les trois paramètres sont définis par:
		$
		\gamma_{j}(b_j,\theta_j) = \gamma_j,\quad  
		\sigma_{j}(b_j,\theta_j) = \sigma_j\,\left(\theta_j/b_j\right)^{\gamma_j},
		\quad
		\mu_{j}(b_j,\theta_j) = \mu_j + \sigma_j\left(\frac{\left(\theta_j/b_j\right)^{\gamma_j} - 1}{\gamma_j}\right).
		$ 
	\end{definition}
	
	
\end{frame}




\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Stationnaire} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Modèle de mélange $G_{\mathbb{P}}(\cdot; \omega)$]
		Le modèle de mélange $G_{\mathbb{P}}(\cdot; \omega)$ est défini pour tout $x\in \mathbb{R}$ par 
		\begin{equation}
			G_{\mathbb{P}}(x; \omega) = \prod_{j = 1}^{p} G_j^{\omega_j}\left(x;\gamma_j(b_j,\theta_j),\, \sigma_j(b_j,\theta_j),\, \mu_j(b_j,\theta_j)\right),
		\end{equation}
		où $\omega = \left(\omega_1,\cdots, \omega_p\right) \in [0,\, 1]^p$ est un vecteur des poids. 
	\end{definition}
	
	
	\begin{block}{Estimation des poids}
		\begin{equation}
			\widehat{\omega} = \argmin_{\omega}\left\{ \sum_{x\in \mathcal{X}, x>x_{n, \alpha}}\, \left[F_{n}(x) - G_{\mathbb{P}}\left(x; \omega \right)\right]^2\right\},
		\end{equation}
		où $F_{n}(\cdot)$ est la fonct. de répartition empirique estimée sur une séq. $\mathcal{X} = \{x_1,\cdots, x_n\}$ d'obs. de $X_t$ de même que le quantile empirique $x_{n, \alpha}$ d'ordre $\alpha > 0.5.$
	\end{block}
	
	
	
	
\end{frame}



\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Stationnaire} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Modèle de mélange $G_{\mathbb{M}}(\cdot; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu})$]
		Soit $G(\cdot)$ la fonction de répartition de la loi GEV. Le modèle de mélange $G_{\mathbb{M}}(\cdot; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu})$ est défini pour tout $x\in \mathbb{R}$ par 
		\begin{equation}
			G_{\mathbb{M}}\left(x; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu} \right) = G\left(x;\sum_{j = 1}^{p}\omega_{\gamma,j}\cdot\gamma_j(b_j,\theta_j), \sum_{j = 1}^{p}\omega_{\sigma,j}\cdot\sigma_j(b_j,\theta_j), \sum_{j = 1}^{p}\omega_{\mu,j}\cdot\mu_j(b_j,\theta_j)\right),
		\end{equation}
		où $\omega_{\gamma} = \left(\omega_{\gamma, 1},\cdots, \omega_{\gamma, p}\right) \in [0,\, 1]^p,$ $\omega_{\sigma} = \left(\omega_{\sigma, 1},\cdots, \omega_{\sigma, p}\right) \in [0,\, 1]^p$ et $\omega_{\mu} = \left(\omega_{\mu, 1},\cdots, \omega_{\mu, p}\right) \in [0,\, 1]^p$ sont trois vecteurs des poids. 
	\end{definition}
	
	\begin{block}{Estimation des poids}
		\begin{equation}
			\left(\widehat{\omega}_{\gamma},
			\widehat{\omega}_{\sigma},
			\widehat{\omega}_{\mu}\right) = \argmin_{\omega_{\gamma},\omega_{\sigma}, \omega_{\mu}}\left\{ \sum_{x\in \mathcal{X}, x>x_{n, \alpha}}\, \left[F_{n}(x) - G_{\mathbb{M}}\left(x; \omega_{\gamma}, \omega_{\sigma}, \omega_{\mu} \right)\right]^2\right\},
		\end{equation}
		où $F_{n}(\cdot)$ est la fonct. de répartition empirique estimée sur une séq. $\mathcal{X} = \{x_1,\cdots, x_n\}$ d'obs. de $X_t$ de même que le quantile empirique $x_{n, \alpha}$ d'ordre $\alpha > 0.5.$
	\end{block}
	
	
\end{frame}





\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Non-Stationnaire} %Each frame should have a title.
	
	\small
	
	Soit $X_t,$ $t=1, 2, \cdots$ une série temporelle \textbf{non-stationnaire} de processus \textbf{inconnu}. 
	
	Soit $Y_t = \left(Y_{1,t}, \cdots,Y_{q,t}\right)$ une série temporelle de $q$ covariables pour la série $X_t.$ 
	
	Soit $x_1,\cdots, x_n$ une séquence de $n$ obs. de la série $X_t.$
	
	On suppose que chaque obs. $x_{\ell}$  est associée à un vecteur de $q$ cov. $y_{\ell} = \left(y_{1,\ell}, \cdots, y_{q,\ell}\right).$ 
	
	
	\begin{block}{}
		\begin{itemize}
			\item Soit $b=\left\{b_j\in \mathbb{N}^{\star},\, j=1,\cdots, p\right\}$ un ensemble de $p$ tailles de blocs assez grandes.
			
			\item Soit $\left(\gamma_j(y_t),\, \sigma_j(y_t),\, \mu_j(y_t)\right)$ le vecteur des paramètres de la loi GEV  nommée $G_j(\cdot\,|\,Y_t = y_t)$ caractérisant la distribution conditionelle des maximums de $b_j$ obs. consécutives de la série $X_t.$
			
			\item Soit $\theta_j\in [0,\,1]$ l'\textbf{indice extremal} associé au seuil défini par le quantile empirique $x_{n, 1/b_j}$ d'ordre $1/b_j$ estimé sur une séquence $\mathcal{X}_n = \{x_1,\cdots, x_n\}$ de la série $X_t.$
			
		\end{itemize}
	\end{block}
	
	\begin{block}{Structure des paramètres}
		\begin{itemize}
			\item $\mu_j(y_t) = \mu_{0,j} + \mu_{1,j}\,f_1(y_t) + \cdots + \mu_{q,j}\,f_q(y_t),$
			
			\item $\sigma_j(y_t) = \exp\left\{\phi_{0,j} + \phi_{1,j}\,g_1(y_t) + \cdots + \phi_{q,j}\,g_q(y_t)\right\},$
			
			\item $\gamma_j(y_t) = \gamma_{0,j} + \gamma_{1,j}\,h_1(y_t) + \cdots + \gamma_{q,j}\,h_q(y_t),$
		\end{itemize}
		où $f_{\ell},\,g_{\ell},\, h_{\ell}$ sont des fonctions continues de supports dans  $\mathbb{R}^q$ et à valeurs dans  $\mathbb{R}.$
	\end{block}
	
	
\end{frame}



\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Non-Stationnaire} %Each frame should have a title.
	
	\small

	
	\begin{definition}[Lois GEV normalisées]
		Soit $y_t = \left(y_{1,t},\cdots, y_{q,t}\right) \in \mathbb{R}^q$ un vecteur constitué des valeurs potentielles des $q$ covariables associées à la série $X_t.$ La loi GEV normalisée $G_j^{\theta_j/b_j}(\cdot\,|\,Y_t = y_t)$ caractérisant la distribution conditionelle des grandes obs. de $X_t$ est la loi GEV $G_j\left(\cdot;\gamma_j(b_j,\theta_j, y_t), \,\sigma_j(b_j,\theta_j,y_t),\, \mu_j(b_j, \theta_j,y_t)\right)$ dont les trois paramètres sont définis par:
		\begin{itemize}
			\item $\gamma_{j}(b_j,\theta_j, y_t) = \gamma_j(y_t)$ (paramètre de forme),
			
			\item $\sigma_{j}(b_j,\theta_j, y_t) = \sigma_j(y_t)\cdot\left(\theta_j/b_j\right)^{\gamma_j(y_t)}$ (paramètre d'échelle),
			
			\item $\mu_{j}(b_j,\theta_j,y_t) = \mu_j(y_t) + \sigma_j(y_t)\cdot\left(\frac{\left(\theta_j/b_j\right)^{\gamma_j(y_t)} - 1}{\gamma_j(y_t)}\right)$ (paramètre de position).
		\end{itemize}

	\end{definition}
	
\end{frame}




\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Non-Stationnaire} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Modèle de mélange $G_{\mathbb{P}}\left(\cdot\,|\,Y_t = y_t;\omega(y_t)\right)$]
		Soit $y_t = \left(y_{1,t},\cdots, y_{q,t}\right) \in \mathbb{R}^q$ un vecteur constitué des valeurs potentielles des $q$ covariables associées à la série $X_t.$ Le modèle de mélange $G_{\mathbb{P}}\left(\cdot\,|\,Y_t = y_t;\omega(y_t)\right)$ est défini pour tout $x\in \mathbb{R}$ par 
		\begin{equation}
			G_{\mathbb{P}}\left(x\,|\,Y_t = y_t;\omega(y_t)\right) = \prod_{j = 1}^{p} G_j^{\omega_j(y_t)}\left(x;\gamma_j(b_j,\theta_j,y_t),\, \sigma_j(b_j,\theta_j,y_t),\, \mu_j(b_j,\theta_j,y_t)\right),
		\end{equation}
		où $\omega(y_t) = \left(\omega_1(y_t),\cdots, \omega_p(y_t)\right) \in [0,\, 1]^p$ est un vecteur des poids.
	\end{definition}
	
	
	\begin{block}{Estimation des poids $\omega_j(y_t)$}
		\begin{equation}
			\widehat{\omega}(y_t) = \argmin_{\omega}\left\{ \sum_{x\in \mathcal{X}, x>x_{n, \alpha}}\, \left[F_{n}(x\,|\,Y_t \in \mathcal{V}(y_t,k)) - G_{\mathbb{P}}\left(x\,|\,Y_t = y_t;\omega(y_t)\right)\right]^2\right\},
		\end{equation}
		où le quantile empirique $x_{n, \alpha}$ d'ordre $\alpha > 0.5$ est estimé sur une séquence $\mathcal{X} = \{x_1, \cdots, x_n\}$ de $X_t$ tandis que la fonct. de répartition empirique $F_{n}\left(\cdot \,|\,Y_t \in \mathcal{V}(y_t,k)\right)$ est  estimée sur la sous seq. $\mathcal{X}(y_t) = \left\{x_{\ell} \in \mathcal{X}:\, y_{\ell} \in \mathcal{V}(y_t,k)\right\}$ dans lequel $\mathcal{V}(y_t,k)$ est l'ens. des $k$-NN de $y_t.$ 
	\end{block}
	
\end{frame}



\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Non-Stationnaire} %Each frame should have a title.
	
	\small
	
	\begin{definition}[Modèle de mélange $G_{\mathbb{M}}(\cdot\,|\,Y_t = y_t; \omega_{\gamma}(y_t), \omega_{\sigma}(y_t), \omega_{\mu}(y_t))$]
		Soit $G(\cdot)$ la fonction de répartition de la loi GEV. 
		Soit $y_t = \left(y_{1,t},\cdots, y_{q,t}\right) \in \mathbb{R}^q$ un vecteur constitué des valeurs potentielles des $q$ covariables associées à la série $X_t.$ 
		Le modèle de mélange $G_{\mathbb{M}}\left(\cdot\,|\,Y_t = y_t; \omega_{\gamma}(y_t), \omega_{\sigma}(y_t), \omega_{\mu}(y_t)\right)$ est défini pour tout $x\in \mathbb{R}$ par 
		\begin{eqnarray}
			G_{\mathbb{M}}\left(x\,|\,Y_t = y_t; \omega_{\gamma}(y_t), \omega_{\sigma}(y_t), \omega_{\mu}(y_t) \right) 
			&=&
			 G\left(x;\sum_{j = 1}^{p}\omega_{\gamma,j}(y_t)\cdot\gamma_j(b_j,\theta_j,y_t),\right.\nonumber\\
			 & &
			 \qquad \,\, \sum_{j = 1}^{p}\omega_{\sigma,j}(y_t)\cdot\sigma_j(b_j,\theta_j,y_t), \nonumber\\
			 & &
			 \qquad \,\, \left. \sum_{j = 1}^{p}\omega_{\mu,j}(y_t)\cdot\mu_j(b_j,\theta_j,y_t)\right),
		\end{eqnarray}
		où $\omega_{\gamma}(y_t) = \left(\omega_{\gamma, 1}(y_t),\cdots, \omega_{\gamma, p}(y_t)\right) \in [0,\, 1]^p,$ $\omega_{\sigma}(y_t) = \left(\omega_{\sigma, 1}(y_t),\cdots, \omega_{\sigma, p}(y_t)\right) \in [0,\, 1]^p$ et $\omega_{\mu}(y_t) = \left(\omega_{\mu, 1}(y_t),\cdots, \omega_{\mu, p}(y_t)\right) \in [0,\, 1]^p$ sont trois vecteurs des poids. 
	\end{definition}
	
	
	
\end{frame}


\begin{frame}
	\frametitle{Mélange des distributions GEV: Cas Non-Stationnaire} %Each frame should have a title.
	
	\small
	
	\begin{block}{Estimation des poids $\omega_{\gamma}(y_t),\, \omega_{\sigma}(y_t),\, \omega_{\mu}(y_t)$}
		\begin{eqnarray}
			\left(\widehat{\omega}_{\gamma}(y_t),\,
			\widehat{\omega}_{\sigma}(y_t),\,
			\widehat{\omega}_{\mu}(y_t)\right) 
			&=& 
			\argmin_{\omega_{\gamma},\omega_{\sigma}, \omega_{\mu}}\left\{ \sum_{x\in \mathcal{X}, x>x_{n, \alpha}}\, \left[F_{n}(x\,|\,Y_t \in \mathcal{V}(y_t,k)) - \right.\right.\nonumber\\ 
			& & \left. \left. G_{\mathbb{M}}\left(x\,|\,Y_t = y_t; \omega_{\gamma}(y_t), \omega_{\sigma}(y_t), \omega_{\mu}(y_t) \right)\right]^2\right\},
		\end{eqnarray}
		où le quantile empirique $x_{n, \alpha}$ d'ordre $\alpha > 0.5$ est estimé sur une séquence $\mathcal{X} = \{x_1, \cdots, x_n\}$ de $X_t$ tandis que la fonct. de répartition empirique $F_{n}\left(\cdot \,|\,Y_t \in \mathcal{V}(y_t,k)\right)$ est  estimée sur la sous seq. $\mathcal{X}(y_t) = \left\{x_{\ell} \in \mathcal{X}:\, y_{\ell} \in \mathcal{V}(y_t,k)\right\}$ dans lequel $\mathcal{V}(y_t,k)$ est l'ens. des $k$-NN de $y_t.$ 
	\end{block}
	
	
	
\end{frame}




\end{document}
